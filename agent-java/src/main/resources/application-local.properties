# ============================================================================
# LLM UI Test Agent - Local Development Profile
# ============================================================================
# Activate: mvn spring-boot:run -Dspring-boot.run.profiles=local
# ============================================================================
# ============================================================================
# Browser Settings (Local)
# ============================================================================
# Non-headless for visual debugging
agent.browser.headless=false
# Slow motion for observing actions
agent.browser.slow-mo=100
# Enable tracing for debugging
agent.browser.tracing-enabled=true
# Enable HAR logging for network analysis
agent.browser.har-enabled=true
# ============================================================================
# Debug Settings
# ============================================================================
# Show LLM prompts in logs for debugging
agent.debug.show-prompts=true
# ============================================================================
# Logging (Verbose for local development)
# ============================================================================
logging.level.agent=DEBUG
logging.level.com.embabel=DEBUG
logging.level.com.microsoft.playwright=INFO
# ============================================================================
# Server Settings
# ============================================================================
server.port=8080
# ============================================================================
# Concurrency (reduced for local)
# ============================================================================
agent.concurrency=1
# ============================================================================
# LLM Model (local - can use Ollama)
# ============================================================================
# Uncomment to use local Ollama instead of cloud APIs
# embabel.models.default-llm=ollama/llama3.2
# spring.ai.ollama.base-url=http://localhost:11434
